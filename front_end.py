import os
from pathlib import Path
import requests
from PIL import Image
from typing import Optional
from types import SimpleNamespace

from qwen_agent.llm.schema import CONTENT, ROLE, USER, Message
from qwen_agent.gui.utils import convert_history_to_chatbot
from qwen_agent.gui.gradio_dep import gr, mgr
from qwen_agent.gui import WebUI
import gradio as gr
from questions import QUESTION_POOL
import random
import json
from collections.abc import Iterator

# ---------- èµ„æº ----------

BASE_DIR = Path(__file__).resolve().parent
bot_logo = BASE_DIR / "bot.jpg"
user_logo = BASE_DIR / "user.jpg"

# ---------- UI é…ç½® ----------

chatbot_config = {
    "agent.avatar": bot_logo,
    "input.placeholder": "Please input your request here",
    'user.name': "me", 
    "user.avatar": user_logo
}

CHAT_MIN_H = 240
CHAT_MAX_H = 1080000
CHAT_STEP_H = 2400

import qwen_agent.gui.utils as qa_utils

qa_utils.TOOL_CALL = """
<details>
<summary>Searching for solutions...</summary>
</details>
""".strip()

qa_utils.TOOL_OUTPUT = """
<details>
<summary>Finishing this step...</summary>
</details>
""".strip()


class AgentWebUI(WebUI):
    """
    è´Ÿè´£æ„å»º Gradio å¸ƒå±€ã€å¯¹æ¥ Qwen-Agent Assistantã€‚
    æŒæœ‰ä¸€ä¸ª OpenVINOAgentPipeline å®ä¾‹ï¼ˆé€šè¿‡ pipeline å±æ€§ä¼ å…¥ï¼‰ã€‚
    """

    def __init__(self, bot, pipeline, chatbot_config: Optional[dict] = None):
        super().__init__(bot, chatbot_config=chatbot_config or {})
        self.pipeline = pipeline
        self.llm_choices = getattr(pipeline, "llm_choices", None) or []
        self.run_kwargs = {}
        self.footer_links = []

    def _build_chatbot(self, messages):
        from qwen_agent.gui.gradio_dep import mgr

        initial = convert_history_to_chatbot(messages=messages)
        initial = self._scrub_chat_pairs(initial)
        return mgr.Chatbot(
            value=initial,
            avatar_images=[self.user_config, self.agent_config_list],
            height=720,
            avatar_image_width=64,
            flushing=False,
            show_copy_button=True,
            latex_delimiters=[
                {"left": "\\(", "right": "\\)", "display": True},
                {"left": "\\begin{equation}", "right": "\\end{equation}", "display": True},
                {"left": "\\begin{align}", "right": "\\end{align}", "display": True},
                {"left": "\\begin{alignat}", "right": "\\end{alignat}", "display": True},
                {"left": "\\begin{gather}", "right": "\\end{gather}", "display": True},
                {"left": "\\begin{CD}", "right": "\\end{CD}", "display": True},
                {"left": "\\[", "right": "\\]", "display": True},
            ],
        )

    def _build_input(self):
        from qwen_agent.gui.gradio_dep import mgr

        return mgr.MultimodalInput(placeholder=self.input_placeholder, elem_id="multimodal-input")

    def _build_asr_audio(self):
        from qwen_agent.gui.gradio_dep import gr

        return gr.Audio(
            label="ğŸ™ï¸ Voice Input",
            sources=["microphone"],
            type="filepath",
            editable=False,
        )
    

    # ---------- ASR ----------

    def _asr_to_input(self, wav_path, current_input):
        """
        ä½¿ç”¨ pipeline.asr_runner å°†è¯­éŸ³è½¬æ–‡å­—ï¼Œå¹¶å†™å› input ç»„ä»¶ã€‚
        """
        gr.Info(f"recognizing speech...", duration=2)
        asr_runner_local = getattr(self.pipeline, "asr_runner", None)

        def _extract_triplet(val):
            if isinstance(val, dict):
                return (
                    val.get("text", "") or "",
                    val.get("files", []) or [],
                    val.get("images", []) or [],
                    dict,
                )
            elif isinstance(val, SimpleNamespace):
                return (
                    getattr(val, "text", "") or "",
                    getattr(val, "files", []) or [],
                    getattr(val, "images", []) or [],
                    SimpleNamespace,
                )
            return ("", [], [], dict)

        def _build_like(orig_type, text, files, images):
            if orig_type is SimpleNamespace:
                return SimpleNamespace(text=text, files=files, images=images)
            return {"text": text, "files": files, "images": images}

        text0, files0, images0, orig_type = _extract_triplet(current_input)

        if not asr_runner_local or not wav_path:
            return _build_like(orig_type, text0, files0, images0)

        try:
            from pathlib import Path as _Path

            text = asr_runner_local(_Path(wav_path))
            print(f"[asr] wav: {wav_path}")
            print(f"[asr] text: {text!r}")
            if not isinstance(text, str) or not text.strip():
                return _build_like(orig_type, text0, files0, images0)
            return _build_like(orig_type, text.strip(), [], [])
        except Exception as e:
            print(f"[asr] error: {type(e).__name__}: {e}")
            return _build_like(orig_type, text0, files0, images0)

    # ---------- æ„å»º Gradio Blocksï¼šä¸ç›´æ¥ launchï¼Œåªè¿”å› demo ----------
    def build_demo(
        self,
        messages: Optional[list[Message]] = None,
        concurrency_limit: int = 10,
        enable_mention: bool = False,
    ):
        """
        æ­å»º Gradio Blocksï¼Œä¸åš launchã€‚
        å¤–éƒ¨é¡¹ç›®å¯ä»¥è‡ªè¡Œè°ƒç”¨ demo.queue().launch(...)ã€‚
        """
        from qwen_agent.gui.gradio_dep import gr, ms

        asr_enabled = bool(getattr(self.pipeline, "asr_runner", None))

        llm_choices = getattr(self, "llm_choices", None) or []
        default_llm = llm_choices[0] if llm_choices else None

        with gr.Blocks(
            title="Agentic PC Manager",
            analytics_enabled=False,
            theme=gr.themes.Soft(primary_hue="blue"),
            css="""
            .button-center {
                display: flex !important;
                align-items: center !important;
                justify-content: center !important;
                min-height: 76px !important;
                padding-top: 20px !important;
                background: transparent !important;
            }
            .button-center button {
                margin: 0 !important;
            }
            footer {
                display: none !important;
            }
            .tool-button.upload-button {
                display: none !important;
            }
            """,
        ) as demo:
            MAX_USER_TURNS = 3  # åªä¿ç•™æœ€è¿‘ 3 è½®å¯¹è¯ä¸Šä¸‹æ–‡

            def _get_role(m):
                try:
                    return m.get("role") if isinstance(m, dict) else getattr(m, "role", None)
                except Exception:
                    return None

            def _trim_history_by_user_turns(hist, max_user_turns: int = MAX_USER_TURNS):
                """
                ä¿ç•™ï¼š
                - æ‰€æœ‰ systemï¼ˆåªä¿ç•™ç¬¬ä¸€ä¸ªä¹Ÿå¯ä»¥ï¼Œè¿™é‡Œä¿ç•™å…¨éƒ¨ä¹Ÿè¡Œï¼‰
                - ä»â€œå€’æ•°ç¬¬ max_user_turns ä¸ª userâ€å¼€å§‹åˆ°ç»“å°¾çš„æ‰€æœ‰æ¶ˆæ¯ï¼ˆå« tool/functionï¼‰
                å¹¶ç¡®ä¿æˆªæ–­åçš„é system ç¬¬ä¸€æ¡ä¸€å®šæ˜¯ userã€‚
                """
                if not isinstance(hist, list):
                    return hist

                sys_msgs = [m for m in hist if _get_role(m) == "system"]
                other = [m for m in hist if _get_role(m) != "system"]

                # ä»æœ«å°¾å¾€å‰æ”¶é›†ï¼Œç›´åˆ°æ”¶é›†åˆ° max_user_turns ä¸ª userï¼ˆåŒ…å«è¯¥ user åŠå…¶ä¹‹åæ‰€æœ‰æ¶ˆæ¯ï¼‰
                collected = []
                user_cnt = 0
                for m in reversed(other):
                    collected.append(m)
                    if _get_role(m) == "user":
                        user_cnt += 1
                        if user_cnt >= max_user_turns:
                            break

                kept = list(reversed(collected))

                # ä¿é™©ï¼šå¦‚æœå¼€å¤´ä¸æ˜¯ userï¼Œå°±ä¸€ç›´ä¸¢åˆ°é‡åˆ°ç¬¬ä¸€ä¸ª userï¼ˆé¿å… 400ï¼‰
                while kept and _get_role(kept[0]) != "user":
                    kept.pop(0)

                return sys_msgs + kept

            def _trim_ctx(cb, hist):
                return cb, _trim_history_by_user_turns(hist)

            def _backend_status():
                # å•ä¸€çœŸç›¸æºï¼šåç«¯ pipeline
                if hasattr(self.pipeline, "get_status"):
                    st = self.pipeline.get_status() or {}
                else:
                    st = {"initialized": bool(getattr(self.pipeline, "initialized", False)),
                        "model_id": getattr(self.pipeline, "model_id", None),
                        "device": getattr(self.pipeline, "device", None)}
                return st
            
            def _agent_run_guard(cb, hist):
                try:
                    st = _backend_status()
                    if not st.get("initialized", False):
                        gr.Warning("Model is not loaded. Auto-load may have failed.")
                        yield cb, hist
                        return

                    ret = self.agent_run(cb, hist)
                    if isinstance(ret, Iterator):
                        yield from ret
                    else:
                        yield ret
                except Exception as e:
                    gr.Warning(f"agent_run failed: {type(e).__name__}: {e}")
                    yield cb, hist

            
            def _sync_ui():
                st = _backend_status()
                ready = bool(st.get("initialized", False))
                model_id = st.get("model_id") or "unloaded"
                device = st.get("device") or "-"
            
                llm_md = f"ğŸ§  **Current LLM:** `{model_id if ready else 'unloaded'}`"
                dev_md = f"ğŸ’» **Current Device:** `{device}`"
            
                u = gr.update(interactive=ready)
            
                # release åç¦ç”¨è¾“å…¥ç­‰æ§ä»¶  
                return (llm_md, dev_md, u, u, u, u, u, u)  # 8 ä¸ªè¾“å‡º

            history = gr.State([])
            backend_status = gr.State({})

            with ms.Application():
                # ---------- é¡¶éƒ¨æ ‡é¢˜ + Release ----------
                with gr.Row():
                    with gr.Column(scale=10):
                        gr.HTML(
                            """
                        <div style="text-align: left; margin-bottom: 10px;">
                            <h1>ğŸ¤– Agentic PC Manager</h1>
                        </div>
                        """
                        )
                    with gr.Column(scale=1, min_width=80):
                        btn_release = gr.Button(
                            "ğŸ—‘ï¸ Release", variant="secondary", size="md", min_width=70
                        )
                        # btn_clear = gr.Button("ğŸ§¹ Clear Chat", variant="secondary", size="md", min_width=70)

                gr.HTML(
                    """
                <div style="text-align: left; margin-bottom: 10px; font-size: 18px;">
                    Agentic chatbot to bridge your intentions with Windows settings and Windows system tools.
                </div>
                """
                )

                # ---------- é¡¶éƒ¨ï¼šLLM + Device + Load ----------
                with gr.Row(variant="compact"):
                    dd_llm = gr.Dropdown(
                        choices=llm_choices,
                        value=default_llm,
                        label="LLM",
                        scale=1,
                    )
                    dd_device = gr.Dropdown(
                        choices=["GPU"],
                        value="GPU",
                        label="Device",
                        scale=1,
                    )
                    with gr.Column(scale=1, elem_classes=["button-center"]):
                        btn_load = gr.Button("ğŸ”„ Load Model", variant="primary")

                # ---------- ä¸­éƒ¨ï¼šChat + Input + (ASR) ----------
                with gr.Column(elem_classes="container", scale=1):
                    chatbot = mgr.Chatbot(
                        value=convert_history_to_chatbot(messages=messages or []),
                        avatar_images=[self.user_config, self.agent_config_list],
                        avatar_image_width=64,
                        flushing=False,
                        show_copy_button=True,
                        latex_delimiters=[
                            {"left": "\\(", "right": "\\)", "display": True},
                            {
                                "left": "\\begin{equation}",
                                "right": "\\end{equation}",
                                "display": True,
                            },
                            {
                                "left": "\\begin{align}",
                                "right": "\\end{align}",
                                "display": True,
                            },
                            {
                                "left": "\\begin{alignat}",
                                "right": "\\end{alignat}",
                                "display": True,
                            },
                            {
                                "left": "\\begin{gather}",
                                "right": "\\end{gather}",
                                "display": True,
                            },
                            {
                                "left": "\\begin{CD}",
                                "right": "\\end{CD}",
                                "display": True,
                            },
                            {"left": "\\[", "right": "\\]", "display": True},
                        ],
                        scale=1,
                    )

                    input = mgr.MultimodalInput(
                        placeholder=self.input_placeholder,
                        scale=0,
                        elem_id="pc_manager_input"
                    )
                    # ------------------ pre-defined questions -------------------
                    def _pick_three_questions_list():
                        pool = QUESTION_POOL[:] if QUESTION_POOL else ["Hello"]
                        if len(pool) >= 3:
                            return random.sample(pool, 3)
                        return (pool + pool + pool)[:3]

                    init_q1, init_q2, init_q3 = _pick_three_questions_list()
                    q1_state = gr.State(init_q1)
                    q2_state = gr.State(init_q2)
                    q3_state = gr.State(init_q3)

                    with gr.Row(variant="compact"):
                        btn_q1 = gr.Button(init_q1, variant="secondary", scale=3, interactive=False)
                        btn_q2 = gr.Button(init_q2, variant="secondary", scale=3, interactive=False)
                        btn_q3 = gr.Button(init_q3, variant="secondary", scale=3, interactive=False)
                        btn_refresh_q = gr.Button("ğŸ”„ Try others", variant="primary", scale=1, interactive=False)

                    def _pick_three_questions():
                        a, b, c = _pick_three_questions_list()
                        return (
                            a, b, c,
                            gr.update(value=a),
                            gr.update(value=b),
                            gr.update(value=c),
                        )

                    btn_refresh_q.click(
                        fn=_pick_three_questions,
                        inputs=None,
                        outputs=[q1_state, q2_state, q3_state, btn_q1, btn_q2, btn_q3],
                        queue=False,
                    )

                    # --------- ASR ----------
                    if asr_enabled:
                        audio = gr.Audio(
                            label="ğŸ™ï¸ Voice Input",
                            sources=["microphone"],
                            type="filepath",
                            editable=False,
                            scale=0,
                        )
                                
                # ---------- åº•éƒ¨çŠ¶æ€ï¼šå½“å‰ LLM & è®¾å¤‡ ----------
                with gr.Row(variant="compact"):
                    current_llm_md = gr.Markdown(
                        "ğŸ§  **Current LLM:** `unloaded`", container=True
                    )
                    current_device_md = gr.Markdown(
                        "ğŸ’» **Current Device:** `-`", container=True
                    )

                # ---------- åº•éƒ¨ï¼šBenchmark ----------
                with gr.Accordion("Benchmark", open=True):
                    with gr.Column():
                        bench_in_tokens = gr.Slider(
                            label="Benchmark: input tokens (prompt length)",
                            minimum=64,
                            maximum=2048,
                            step=64,
                            value=256,
                            interactive=True,
                        )
                        bench_out_tokens = gr.Slider(
                            label="Benchmark: output max_new_tokens",
                            minimum=16,
                            maximum=512,
                            step=16,
                            value=128,
                            interactive=True,
                        )
                        bench_btn = gr.Button("ğŸ“Š Run Benchmark", variant="primary")
                        bench_result = gr.Markdown("**tokens/s:** -")

                # --------- UI lock targets (disable during generation) ----------
                lock_targets = [
                    input,
                    btn_q1, btn_q2, btn_q3, btn_refresh_q,
                    bench_btn, bench_in_tokens, bench_out_tokens,
                    btn_release, btn_load,
                    dd_llm,
                ]
                if asr_enabled:
                    lock_targets.append(audio)

                def _ensure_backend(selected_llm, selected_device):
                    # è¿™ä¸€æ­¥åœ¨ submit æ—¶è§¦å‘ï¼šè‡ªåŠ¨ load / reload
                    selected_device = "GPU"
                    try:
                        if hasattr(self.pipeline, "ensure_loaded"):
                            st = self.pipeline.ensure_loaded(selected_llm, selected_device)
                        else:
                            # fallback
                            if not getattr(self.pipeline, "initialized", False):
                                self.pipeline.initialize(device=selected_device)
                            st = {"initialized": getattr(self.pipeline, "initialized", False),
                                "model_id": getattr(self.pipeline, "model_id", None),
                                "device": getattr(self.pipeline, "device", None)}
                        return st
                    except Exception as e:
                        gr.Warning(f"Auto-load failed: {type(e).__name__}: {e}")
                        try:
                            return self.pipeline.get_status()
                        except Exception:
                            return {"initialized": False, "model_id": None, "device": None}

                def _apply_status(st):
                    ready = bool((st or {}).get("initialized", False))
                    model_id = (st or {}).get("model_id") or "unloaded"
                    device = (st or {}).get("device") or "-"

                    llm_md = f"ğŸ§  **Current LLM:** `{model_id if ready else 'unloaded'}`"
                    dev_md = f"ğŸ’» **Current Device:** `{device}`"

                    u = gr.update(interactive=ready)
                    return (llm_md, dev_md, u, u, u, u, u, u)  # 8 outputs
                
                def _disable_user_controls():
                    # âœ… è¾“å‡ºè¿‡ç¨‹ä¸­ï¼Œç¦ç”¨ï¼šè¾“å…¥æ¡†ï¼ˆç›¸å½“äºç¦ç”¨ submitï¼‰ã€é¢„è®¾é—®é¢˜ã€æ¢ä¸€æ‰¹ã€benchmark
                    u = gr.update(interactive=False)
                    return (u, u, u, u, u, u)  # input, q1, q2, q3, refresh_q, bench_btn

                def _enable_user_controls():
                    # âœ… è¾“å‡ºç»“æŸåï¼Œæ ¹æ®åç«¯çœŸå®çŠ¶æ€æ¢å¤å¯ç”¨æ€§
                    st = _backend_status()
                    ready = bool(st.get("initialized", False))
                    u = gr.update(interactive=ready)
                    return (u, u, u, u, u, u)

                input.submit(
                    fn=self.add_text,
                    inputs=[input, chatbot, history],
                    outputs=[input, chatbot, history],
                    queue=False,
                ).then(
                    fn=_trim_ctx,
                    inputs=[chatbot, history],
                    outputs=[chatbot, history],
                    queue=False,
                ).then(
                    fn=_ensure_backend,
                    inputs=[dd_llm, dd_device],
                    outputs=[backend_status],
                    queue=True,
                ).then(
                    fn=_apply_status,
                    inputs=[backend_status],
                    outputs=[current_llm_md, current_device_md, input, btn_q1, btn_q2, btn_q3, btn_refresh_q, bench_btn],
                    queue=False,
                ).then(
                    fn=_disable_user_controls,
                    inputs=None,
                    outputs=[input, btn_q1, btn_q2, btn_q3, btn_refresh_q, bench_btn],
                    queue=False,
                ).then(
                    _agent_run_guard,
                    inputs=[chatbot, history],
                    outputs=[chatbot, history],
                ).then(
                    fn=_trim_ctx,
                    inputs=[chatbot, history],
                    outputs=[chatbot, history],
                    queue=False,
                ).then(
                    self.flushed, None, [input]
                ).then(
                    fn=_enable_user_controls,
                    inputs=None,
                    outputs=[input, btn_q1, btn_q2, btn_q3, btn_refresh_q, bench_btn],
                    queue=False,
                )


                # ---------- ASR è‡ªåŠ¨å†™å…¥ ----------
                if asr_enabled:
                    audio.change(
                        fn=self._asr_to_input,
                        inputs=[audio, input],
                        outputs=[input],
                        queue=False,
                    )

                # --------- random questions -------------
                def _mm_from_text(t: str):
                    return {"text": t, "files": [], "images": []}
                
                def _bind_quick_btn(btn, q_state):
                    btn.click(
                        fn=_mm_from_text,
                        inputs=[q_state],
                        outputs=[input],
                        queue=False,
                    ).then(
                        self.add_text,
                        inputs=[input, chatbot, history],
                        outputs=[input, chatbot, history],
                        queue=False,
                    ).then(
                        fn=_trim_ctx,
                        inputs=[chatbot, history],
                        outputs=[chatbot, history],
                        queue=False,
                    ).then(
                        fn=_ensure_backend,
                        inputs=[dd_llm, dd_device],
                        outputs=[backend_status],
                        queue=True,
                    ).then(
                        fn=_apply_status,
                        inputs=[backend_status],
                        outputs=[current_llm_md, current_device_md, input, btn_q1, btn_q2, btn_q3, btn_refresh_q, bench_btn],
                        queue=False,
                    ).then(
                        fn=_disable_user_controls,  # å¼€å§‹è¾“å‡ºå‰ç¦ç”¨
                        inputs=None,
                        outputs=[input, btn_q1, btn_q2, btn_q3, btn_refresh_q, bench_btn],
                        queue=False,
                    ).then(
                        _agent_run_guard,
                        inputs=[chatbot, history],
                        outputs=[chatbot, history],
                    ).then(
                        fn=_trim_ctx,
                        inputs=[chatbot, history],
                        outputs=[chatbot, history],
                        queue=False,
                    ).then(
                        self.flushed, None, [input]
                    ).then(
                        fn=_enable_user_controls,   # è¾“å‡ºç»“æŸåæ¢å¤
                        inputs=None,
                        outputs=[input, btn_q1, btn_q2, btn_q3, btn_refresh_q, bench_btn],
                        queue=False,
                    )

                
                _bind_quick_btn(btn_q1, q1_state)
                _bind_quick_btn(btn_q2, q2_state)
                _bind_quick_btn(btn_q3, q3_state)

                # ---------- é¡¶éƒ¨ Load / Release æŒ‰é’®é€»è¾‘ ----------

                def _do_release():
                    try:
                        self.pipeline.release()
                    except Exception as e:
                        gr.Warning(f"release fails: {type(e).__name__}: {e}")
                    gr.Info(f"model released", duration=2)
                    return _sync_ui()

                def _do_load(selected_llm, selected_device):
                    gr.Info(f"loading {selected_llm} on {selected_device}...", duration=5)
                    # selected_llm ä»…ç”¨äºæ˜¾ç¤º/é€‰æ‹©ï¼›çœŸå®çŠ¶æ€ä»¥ pipeline ä¸ºå‡†
                    selected_device = "GPU"
                    if not selected_device:
                        selected_device = "GPU"
                    try:
                        self.pipeline.initialize(device=selected_device)
                    except Exception as e:
                        gr.Warning(f"load fails: {type(e).__name__}: {e}")
                    gr.Info(f"{selected_llm} on {selected_device} loaded", duration=2)
                    return _sync_ui()
                
                btn_release.click(
                    _do_release,
                    inputs=None,
                    outputs=[current_llm_md, current_device_md, input, btn_q1, btn_q2, btn_q3, btn_refresh_q, bench_btn],
                    queue=True,
                )
                
                btn_load.click(
                    _do_load,
                    inputs=[dd_llm, dd_device],
                    outputs=[current_llm_md, current_device_md, input, btn_q1, btn_q2, btn_q3, btn_refresh_q, bench_btn],
                    queue=True,
                )

                def _do_bench(in_tokens, out_tokens):
                    gr.Info("benchmark executing", duration=5)
                    bench_cb = getattr(self.pipeline, "benchmark", None)
                    if bench_cb is None:
                        gr.Warning("no benchmark backend is provided")
                        return "**tokens/s:** benchmark backend not available"
                
                    ret = bench_cb(int(in_tokens), int(out_tokens))
                
                    if not isinstance(ret, dict) or not ret.get("ok"):
                        err = ret.get("error") if isinstance(ret, dict) else str(ret)
                        gr.Warning(f"benchmark failed: {err}")
                        return f"**tokens/s:** n/a\n\n_error:_ {err}"
                
                    tps = ret.get("tokens_per_s", 0.0)
                    ttft = ret.get("ttft_s", None)
                    tpot = ret.get("tpot_s", None)
                
                    in_tgt = ret.get("input_tokens_target")
                    in_act = ret.get("input_tokens_actual")
                    out_tgt = ret.get("output_tokens_target")
                    warm = ret.get("warmup_iters")
                    iters = ret.get("iters")
                
                    lines = []
                    lines.append(f"**tokens/s:** {tps:.2f}")
                    if ttft is not None:
                        lines.append(f"- TTFT = {ttft*1000:.1f} ms")
                    if tpot is not None:
                        lines.append(f"- TPOT = {tpot*1000:.2f} ms/token")
                    lines.append(f"- input_tokens  (target/actual) = {in_tgt}/{in_act}")
                    lines.append(f"- output_tokens (max_new_tokens)= {out_tgt}")
                    lines.append(f"- warmup_iters = {warm}")
                    lines.append(f"- repeat_iters = {iters}")
                    gr.Info("benchmark finished",duration=2)
                    return "\n".join(lines)

                bench_btn.click(
                    _do_bench,
                    inputs=[bench_in_tokens, bench_out_tokens],
                    outputs=[bench_result],
                    queue=True,
                )
            
            demo.load(
                fn=_sync_ui,
                inputs=None,
                outputs=[current_llm_md, current_device_md, input, btn_q1, btn_q2, btn_q3, btn_refresh_q, bench_btn], 
                queue=False,
            )
            demo.load(
                fn=_pick_three_questions,
                inputs=None,
                outputs=[q1_state, q2_state, q3_state, btn_q1, btn_q2, btn_q3],
                queue=False,
            )

        # ä¸åœ¨è¿™é‡Œ queue / launchï¼Œç”±å¤–éƒ¨é¡¹ç›®è‡ªå·±å†³å®š
        return demo

    # ä¸æ˜¾ç¤ºå·¥å…·é€‰æ‹©å™¨
    def _create_agent_plugins_block(self, agent_index=0):
        from qwen_agent.gui.gradio_dep import gr

        return gr.Markdown(visible=False)


# ======================================================================
# å¯¹å¤–ï¼šGradio å°è£…ç±» + demo è¾…åŠ©å‡½æ•°
# ======================================================================


class AgentGradio:
    """
    å¯¹å¤–æš´éœ²çš„ Gradio å°è£…ç±»:

    - æŒæœ‰ä¸€ä¸ª OpenVINOAgentPipeline å®ä¾‹ï¼ˆself.pipelineï¼‰
    - æä¾›ç»Ÿä¸€æ¥å£ï¼š
        - initialize(model_dir, device)
        - release()
        - build_demo() -> gr.Blocks
    """

    def __init__(self, agent_pipeline) -> None:
        self.pipeline = agent_pipeline
        self.multilingual = True  # ä¸ dummy_example é£æ ¼ä¿æŒä¸€è‡´
        self._ui: AgentWebUI | None = None

    def initialize(
        self,
        model_dir: Optional[str | Path] = None,
        device: str = "GPU",
        no_export: bool = False,
    ):
        """
        å¯¹å¤–ç»Ÿä¸€åˆå§‹åŒ–å…¥å£ã€‚
        å¤–éƒ¨é¡¹ç›®å¯ä»¥é€‰æ‹©æå‰åˆå§‹åŒ–ï¼ˆä¹Ÿå¯ä»¥é€šè¿‡ UI æŒ‰é’®å†åˆå§‹åŒ–ï¼‰ã€‚
        """
        if model_dir is not None:
            # å…è®¸å¤–éƒ¨è¦†ç›– model_dir
            self.pipeline.model_dir = Path(model_dir)

        if not self.pipeline.initialized:
            self.pipeline.initialize(device=device, no_export=no_export)

    def release(self):
        """
        å¯¹å¤–ç»Ÿä¸€é‡Šæ”¾æ¥å£ã€‚
        """
        if self.pipeline.initialized:
            self.pipeline.release()

    def build_demo(self, concurrency_limit: int = 10):
        """
        æ„å»º Gradio demoï¼ˆä¸ launchï¼‰ã€‚
        """
        if self._ui is None:
            # ç”¨ pipeline é‡Œå·²ç»æ„é€ å¥½çš„ Assistant
            self._ui = AgentWebUI(
                bot=self.pipeline.assistant,
                pipeline=self.pipeline,
                chatbot_config=chatbot_config,
            )
        return self._ui.build_demo(concurrency_limit=concurrency_limit)


def agent_demo(agentgr: AgentGradio):
    return agentgr.build_demo()